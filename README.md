ğŸ” Multi-Level Sentiment Analysis
This project presents a tiered approach to sentiment analysis, offering three progressively more powerful models â€” from lightweight binary classification to fine-tuned transformer-based emotion detection.

ğŸ“Š **Overview**

| Model    | Dataset      | Type                         | Class Count | Use Case                                             | Accuracy | Computational Cost |
|----------|--------------|------------------------------|--------------|------------------------------------------------------|----------|---------------------|
| Model 1  | Sentiment140 | Binary (Positive/Negative)   | 2            | Real-time or low-resource applications               | ~79%     | ğŸŸ¢ Low              |
| Model 2  | GoEmotions   | Multi-class (Emotions)       | 27           | Moderate applications with richer emotion detection  | ~43%     | ğŸŸ¡ Low to Medium    |
| Model 3  | GoEmotions   | Multi-class (Emotions)       | 27           | High-accuracy, nuanced sentiment analysis            | ~92%     | ğŸ”´ High             |


ğŸ§  Model Breakdown


ğŸ”¹ Model 1: Logistic Regression (Binary Classification)
Dataset: Sentiment140

Task: Classifies sentiment as either Positive or Negative.

Model: TF-IDF vectorization + Logistic Regression

Use Case: Best for scenarios with:

Low latency requirements

Limited computational power (e.g., mobile, edge devices)

Real-time filtering or monitoring

Accuracy: ~77% (baseline)

Limitations: Cannot distinguish nuanced emotions or sarcasm, negation handling is a bit weak.



ğŸ”¸ Model 2: XGBoost (Emotion Classification)
Dataset: GoEmotions

Task: Classifies text into 27 emotion categories (e.g., happy, angry, surprised)

Model: TF-IDF + XGBoost classifier

Use Case: Ideal for:

Chatbots or virtual assistants

Emotion-aware product feedback analysis

Applications where training time is less critical than inference speed




ğŸ”» Model 3: Fine-tuned BERT (Emotion Classification)
Dataset: GoEmotions

Task: Emotion classification using fine-tuned BERT

Use Case: Perfect for:

Research and high-stakes NLP applications

Products with backend GPU/TPU support

Psychological or sociological analysis of language

Pros:

Captures nuanced meanings, sarcasm, and context

State-of-the-art results on multi-class sentiment tasks

Cons:

High training/inference cost

Requires GPU for practical deployment





ğŸ“Œ Notes
TF-IDF vectorization is used in both Models 1 and 2.

All models follow a standard pipeline: Preprocessing â†’ Vectorization â†’ Training â†’ Evaluation â†’ Prediction

Preprocessing includes but is not limited to stop word removal, URL/mention stripping, punctuation cleanup, and lemmatization.

ğŸ¤– Future Enhancements
Add live web demo

Model ensemble to combine predictions

Sarcasm detection layer for better classification
